\chapter{Related Works}
\label{chap:related_works}

In this chapter, we survey the key literature relevant to this thesis. We begin with the foundational works that introduced coordinate descent (CD) methods for optimization, tracing their origins and early theoretical developments. We then discuss extensions of CD to distributed and parallel settings, highlighting algorithms designed to scale across multiple nodes. Finally, we examine how CD approaches are increasingly applied to address contemporary challenges in machine learning, particularly the memory and computational demands of training large language models (LLMs).

\paragraph{Coordinate Descent} Coordinate descent (CD) methods are fundamental approaches for smooth unconstrained minimization \cite{nesterov2012efficiency, wright2015coordinate}. Their advantage is simplicity: a single coordinate (or block) is selected and optimized, decomposing a high-dimensional problem into simpler subproblems. Variants include sequential cyclic CD and randomized CD (RCDM), the latter often converging faster in practice. This flexibility makes CD appealing for problems with cheap partial derivatives or a separable structure.
\paragraph{Fundamental Analysis of Randomized Coordinate Descent}
Nesterov \cite{nesterov2012efficiency} provided the foundational modern analysis for randomized coordinate descent (RCDM), rigorously establishing its convergence for smooth convex optimization problems. While the worst-case complexity of cyclic CD can be poor, Nesterov showed that RCDM is efficient in expectation. For a convex function that is $L_{\max}$-smooth per coordinate (where $L_{\max}$ is the largest coordinate-wise Lipschitz constant of $\nabla f$), RCDM with step size $1/L_{\max}$ achieves an expected convergence rate of $\mathbb{E}[f(x_k) - f(x^*)] \leq \frac{n}{k} C$, where $n$ is the number of the coordinate blocks (if each coordiante block be one idmentionl it reduce to the problem dimension), $x_k$ is the $k$-th iterate, $x^*$ is an optimal solution, and $C = \frac{1}{2} \|x_0 - x^*\|^2_{L}$ is a constant depending on the initial distance to the optimum measured in the $L$-weighted norm $\|\cdot\|_L$. This requires $O(n/\epsilon)$ iterations to reach $\epsilon$-accuracy (i.e., to make $\mathbb{E}[f(x_k) - f(x^*)] \le \epsilon$). Although this is $n$ times more iterations than full gradient descent, each iteration is $n$ times cheaper (assuming separable computation), making RCDM computationally viable. For $\mu$-strongly convex functions (with strong convexity parameter $\mu>0$), the rate becomes linear: $\mathbb{E}[f(x_k) - f^*] \leq \left(1 - \frac{\mu}{n L_{\max}}\right)^k (f(x_0) - f^*)$.
\paragraph{Parallel and Accelerated Coordinate Descent}
A major line of research focused on parallelizing and accelerating RCDM. Richt√°rik and colleagues extended the analysis to randomized block-coordinate descent (RBCD) for composite optimization $F(x) = f(x) + \Psi(x)$, where $\Psi$ is a simple, block-separable regularizer (e.g., $\ell_1$-norm) \cite{richtarik2014iteration}. This was crucial for applying CD to problems like LASSO, showing $O(n/\epsilon)$ iteration complexity and linear convergence for strongly convex objectives. The first formal analysis of *parallel* coordinate descent (PCDM) \cite{richtarik2016parallel} introduced a method where a random subset of blocks $\hat{S}$ is sampled and updated in parallel. This is inherently a distributed method. The analysis showed that the convergence rate scales with the number of parallel updates $\tau = \mathbb{E}[|\hat{S}|]$. For $\mu$-strongly convex, $L$-smooth functions, PCDM converges linearly in expectation, $\mathbb{E}[f(x_k) - f^*] \leq (1 - \gamma)^k C$, with a rate $\gamma \approx \frac{\tau}{n} \frac{\mu}{L}$ ($\mu$ and $L$ here are smallest and biggest eigen values of the hessian, respectively). This provided strong theoretical evidence for the scalability of parallel CD. A key challenge was achieving Nesterov's optimal $O(1/k^2)$ accelerated rate without resorting to full-dimensional vector operations that negate CD's per-iteration cheapness. The APPROX method \cite{fercoq2015accelerated} was the first to be simultaneously accelerated, parallel, and proximal-capable. It achieves an expected convergence rate of $\mathbb{E}[F(x_k) - F^*] \leq \frac{C}{(k+1)^2}$ (where $C$ depends on problem structure and initial distance), resolving a major bottleneck for CD methods.
\paragraph{Criticisms and Resurgence in Deep Learning}
Despite this theoretical success, CD methods were largely eschewed for training deep neural networks in the 2010s. A primary criticism was the incompatibility with backpropagation \cite{rumelhart1986learning}, where computing a partial gradient for an early layer might require almost a full backward pass, negating the computational savings. The rise of data-parallel methods like local SGD \cite{pmlr-v54-mcmahan17a} and Federated Learning \cite{mcmahan2017communication}, which partition data instead of parameters, further shifted the research focus. However, the advent of massive large language models (LLMs) has triggered a resurgence in BCD. Optimizers like Adam \cite{kingma2014adam} maintain states for *every* parameter, tripling the model's memory footprint and making fine-tuning prohibitive on commodity hardware. BCD, as a form of parameter-parallelism, offers a compelling solution by optimizing only a subset of parameters (e.g., a few layers) at a time, drastically reducing the per-node optimizer state memory.
\paragraph{Distributed CD and Dual Methods}
Another branch of CD research explores its use in data-distributed settings, often via a dual formulation. The CoCoA framework \cite{jaggi2014communication} is a prominent example. In CoCoA, the problem is formulated in the dual, and data is partitioned across $K$ worker nodes. Each worker, in parallel, performs multiple local iterations of a dual coordinate ascent optimizer (like SDCA) on its local data. Workers then communicate updates for aggregation. This framework, which combines distributed computation, coordinate ascent, and local updates, is a direct precursor to modern federated optimization and was shown to inherit the linear convergence of the local optimizer. Other related works like PASSCoDe \cite{hsieh2015passcode} and CoCoA+ \cite{smith2018cocoa} further refined this data-distributed dual approach.
\paragraph{Block Coordinate Descent for LLMs}
The renewed interest in BCD is best seen in its recent applications to LLM adaptation. \textbf{BAdam} \cite{luo2024badam} is a memory-efficient optimizer that applies the Adam update to only one block of parameters at a time, combining Adam's rapid convergence with BCD's memory savings. \textbf{LISA} \cite{pan2024lisa} (Layerwise Importance Sampling) implements BCD explicitly: it randomly samples a subset of layers, freezes the others, and performs an optimizer step only on the active layers. This dramatically cuts memory usage, enabling the fine-tuning of massive models like LLaMA-2-70B on a single consumer GPU. These applications show BCD has found a critical new role in addressing the memory bottlenecks of modern large-scale models.
